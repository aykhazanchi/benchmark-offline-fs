{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199168a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --pre pandas==2.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import boto3\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab08149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3712773",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75842056",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"35GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"pit-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n",
    "session = boto3.Session( aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# HDFS Uploads\n",
    "HOPS_HOST=''\n",
    "HOPS_API_KEY=''\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083786b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(limit, offset):\n",
    "    file_path=f's3://{BUCKET}/taxidata_cleaned/*.parquet'\n",
    "    raw_data = con.execute(f\"SELECT * FROM read_parquet('{file_path}') LIMIT {limit};\").df()\n",
    "    # Add row_id to raw_data\n",
    "    raw_data['row_id'] = range(offset, offset + len(raw_data))\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_ts(df, ts_column, start_date, end_date):\n",
    "    if ts_column and start_date:\n",
    "        df = df[df[ts_column] >= start_date]\n",
    "    if ts_column and end_date:\n",
    "        df = df[df[ts_column] < end_date]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickup_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_pickup_datetime']).dt.floor('15min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    pickup_features = (\n",
    "        df.groupby(['pu_location_id', 'pu_borough', 'window'])\n",
    "        .agg(\n",
    "            mean_fare_window_1h_pickup_zip=('fare_amount', 'mean'),\n",
    "            count_trips_window_1h_pickup_zip=('fare_amount', 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'pu_location_id': 'location_id', 'pu_borough': 'borough', 'window': 'ts'})\n",
    "    )\n",
    "    pickup_features['row_id'] = pickup_features.reset_index().index\n",
    "    row_id = pickup_features.pop('row_id')\n",
    "    pickup_features.insert(0, 'row_id', row_id)\n",
    "\n",
    "    return pickup_features\n",
    "\n",
    "def dropoff_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_dropoff_datetime']).dt.floor('30min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    dropoff_features = (\n",
    "        df.groupby(['do_location_id', 'do_borough', 'window'])\n",
    "        .agg(count_trips_window_30m_dropoff_zip=('do_borough', 'count'))\n",
    "        .reset_index()\n",
    "        .rename(columns={'do_location_id': 'location_id', 'do_borough': 'borough', 'window': 'ts'})\n",
    "    )\n",
    "    dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts'])\n",
    "    dropoff_features['dropoff_is_weekend'] = dropoff_features['ts'].dt.dayofweek.isin([5, 6])\n",
    "    dropoff_features['row_id'] = dropoff_features.reset_index().index\n",
    "    row_id = dropoff_features.pop('row_id')\n",
    "    dropoff_features.insert(0, 'row_id', row_id)\n",
    "\n",
    "    return dropoff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = [1,2,5,10]\n",
    "\n",
    "for sf in scale_factor:\n",
    "    raw_features_limit = sf * 1000000 # This is used for transformation of pickup and dropoff features\n",
    "    raw_data = get_raw_data(raw_features_limit, 0)\n",
    "#     pickup_features\n",
    "    pickup_features = pickup_features_fn(\n",
    "        df=raw_data,\n",
    "        ts_column=\"tpep_pickup_datetime\",\n",
    "        start_date=datetime(2011, 1, 1),\n",
    "        end_date=datetime(2023, 1, 31),\n",
    "    )\n",
    "    pickup_features['ts'] = pd.to_datetime(pickup_features['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     dropoff_features\n",
    "    dropoff_features = dropoff_features_fn(\n",
    "        df=raw_data,\n",
    "        ts_column=\"tpep_dropoff_datetime\",\n",
    "        start_date=datetime(2011, 1, 1),\n",
    "        end_date=datetime(2023, 1, 31),\n",
    "    )\n",
    "    dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "    pickup_fg = fs.get_or_create_feature_group(\n",
    "        name=f\"pit_pickup_features_{sf}\",\n",
    "        version=1,\n",
    "        primary_key=[\"row_id\"],\n",
    "        event_time=[\"ts\"],\n",
    "        online_enabled=False,\n",
    "        description=\"NYC Taxi data pickup features\"\n",
    "    )\n",
    "    pickup_fg.insert(pickup_features, write_options={\"wait_for_job\" : True})\n",
    "    dropoff_fg = fs.get_or_create_feature_group(\n",
    "        name=f\"pit_dropoff_features_{sf}\",\n",
    "        version=1,\n",
    "        primary_key=[\"row_id\"],\n",
    "        event_time=[\"ts\"],\n",
    "        online_enabled=False,\n",
    "        description=\"NYC Taxi data dropoff features\")\n",
    "    dropoff_fg.insert(dropoff_features, write_options={\"wait_for_job\" : True})\n",
    "    \n",
    "    if raw_features_limit < 5000000:\n",
    "        batch_read_limit = 1000000 # Ingest 1M at a time for SF 1 and 2\n",
    "    else:\n",
    "        batch_read_limit = 5000000 \n",
    "        \n",
    "    offset = 0\n",
    "    total_rows = sf * 1000000\n",
    "\n",
    "    while offset < total_rows:\n",
    "        raw_data = get_raw_data(batch_read_limit, offset)\n",
    "        raw_fg = fs.get_or_create_feature_group(\n",
    "            name=f\"pit_raw_features_{sf}\",\n",
    "            version=1,\n",
    "            primary_key=[\"row_id\"],            \n",
    "            event_time=[\"tpep_pickup_datetime\"],\n",
    "            online_enabled=False,\n",
    "            description=\"NYC Taxi data raw data features\")\n",
    "        raw_fg.insert(raw_data, write_options={\"wait_for_job\" : True})\n",
    "        offset += batch_read_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f883c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0cf42b9",
   "metadata": {},
   "source": [
    "## Benchmark PIT Correct Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "scale_factor = [1, 2, 5, 10]\n",
    "\n",
    "for sf in scale_factor:\n",
    "    # Retrieval\n",
    "    pickup_fg = fs.get_feature_group(\n",
    "        name=f\"pit_pickup_features_{sf}\",\n",
    "        version=1\n",
    "    )\n",
    "    dropoff_fg = fs.get_feature_group(\n",
    "        name=f\"pit_dropoff_features_{sf}\",\n",
    "        version=1\n",
    "    )\n",
    "    raw_fg = fs.get_feature_group(\n",
    "        name=f\"pit_raw_features_{sf}\",\n",
    "        version=1\n",
    "    )\n",
    "    # PIT JOIN\n",
    "    start = time.time()\n",
    "    df = raw_fg.select_all()\\\n",
    "                .join(pickup_fg.select_all(), left_on=[\"pu_location_id\"], right_on=[\"location_id\"], prefix=\"pf_\")\\\n",
    "                .join(dropoff_fg.select_all(), left_on=[\"do_location_id\"], right_on=[\"location_id\"], prefix=\"df_\")\\\n",
    "                .read()\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"PIT JOIN - SF {sf}: {time_taken}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9903cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b5563bf",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54082c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fg.delete()\n",
    "dropoff_fg.delete()\n",
    "raw_fg.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756ab0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc44899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0169c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
