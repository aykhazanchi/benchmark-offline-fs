{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa41fb0-2cc3-45a3-bfd2-cba4de8ca94c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --pre duckdb && pip install --pre pandas==2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2572496-ac31-4941-a93f-11e91108de43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.inputs import TableFormatEnum\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import importlib\n",
    "import logging\n",
    "\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c7cd1-1af7-477b-b51c-e4a4e63c22e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_version = sagemaker.__version__\n",
    "major, minor, patch = sm_version.split('.')\n",
    "if int(major) < 2 or int(minor) < 125:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker==2.125.0'])\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed3ddd-ada1-4372-bf37-42100eac67aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5368b12-61f9-4a6e-91c6-c5395cd4fb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "logger.info(f'Default S3 bucket = {default_bucket}')\n",
    "prefix = 'sagemaker-feature-store'\n",
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f46fc3-68f2-4920-a6df-3fff30e52034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84825f50-e41f-4f77-acde-6827d0649adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"45GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"fg-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n",
    "session = boto3.Session(aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# HDFS Uploads\n",
    "HOPS_HOST=''\n",
    "HOPS_API_KEY=''\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee18b0-7646-48d6-82f0-0be1e6114872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d9b0a-e765-4e1e-a21c-7eb7366956ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fc67e-c388-4350-b19f-ccd147b374b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_feature_data(limit, offset):\n",
    "    lim = limit\n",
    "    off = offset\n",
    "    query = f'''\n",
    "        CREATE \n",
    "        OR REPLACE VIEW taxidata \n",
    "        AS\n",
    "        SELECT \n",
    "            tpep_pickup_datetime,\n",
    "            pu_location_id,\n",
    "            pu_borough,\n",
    "            pu_svc_zone,\n",
    "            pu_zone\n",
    "        FROM \n",
    "            read_parquet([\n",
    "                's3://{BUCKET}/taxidata_cleaned/2011.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2012.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "            ])\n",
    "    '''\n",
    "    con.execute(query)\n",
    "    raw_data = con.execute(f\"SELECT * FROM taxidata LIMIT {lim} OFFSET {off}\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    # Event time data type required as String by Sagemaker \n",
    "    raw_data['tpep_pickup_datetime'] = pd.to_datetime(raw_data['tpep_pickup_datetime']).dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bae3c-90e9-46df-ad23-79369f49a138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        logger.info(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(10)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    logger.info(f'FeatureGroup {feature_group.name} was successfully created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9adf82-c62b-4ebb-81cc-95abdc34a407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78be0f-326a-47b8-9761-d249d5086084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "From documentation: \n",
    "Amazon SageMaker Feature Store supports the AWS Glue and Apache Iceberg table formats for the offline store. \n",
    "You can choose the table format when youâ€™re creating a new feature group.\n",
    "\n",
    "Using Apache Iceberg for storing features accelerates model development by enabling faster query performance when extracting ML training datasets,\n",
    "taking advantage of Iceberg table compaction. Depending on the design of your feature groups and their scale, you can experience training query \n",
    "performance improvements of 10x to 100x by using this new capability.\n",
    "'''\n",
    "\n",
    "table_format_param = 'ICEBERG' # or 'GLUE'\n",
    "if table_format_param == 'ICEBERG':\n",
    "    table_format = TableFormatEnum.ICEBERG\n",
    "else:\n",
    "    table_format = TableFormatEnum.GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da281ee-bf61-4a0d-91b9-c39d4a882a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FG\n",
    "scale_factor = [50]\n",
    "\n",
    "# read 10 rows into df and use this to load feature definitions\n",
    "features_schema_df = read_feature_data(10, 0)\n",
    "features_schema_df = features_schema_df.astype({'tpep_pickup_datetime': 'string'})\n",
    "\n",
    "for sf in scale_factor:\n",
    "    pickup_features_group_name = f'pickup_features_{sf}'\n",
    "    pickup_features_group = FeatureGroup(\n",
    "        name=pickup_features_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    pickup_features_group.load_feature_definitions(data_frame=features_schema_df)\n",
    "    try:\n",
    "        pickup_features_group.create(\n",
    "            s3_uri=f's3://{default_bucket}/{prefix}/',\n",
    "            record_identifier_name='row_id',\n",
    "            event_time_feature_name='tpep_pickup_datetime',\n",
    "            role_arn=role,\n",
    "            enable_online_store=False,\n",
    "            table_format=table_format\n",
    "        )\n",
    "        wait_for_feature_group_creation_complete(pickup_features_group)\n",
    "    except:\n",
    "        print(f\"Feature group {pickup_features_group_name} already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4220932-f7c1-4b2a-b299-e65ae738cd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_factor = [50] # Number of millions of rows to scale \n",
    "limit = 5000000 # Get 10M at a time so it's faster\n",
    "\n",
    "for sf in scale_factor:\n",
    "    offset = 0\n",
    "    total_rows = sf * 1000000  # Millions\n",
    "    while offset < total_rows:\n",
    "        pickup_features_group_name = f'pickup_features_{sf}'\n",
    "        pickup_fg = FeatureGroup(\n",
    "            name=pickup_features_group_name,\n",
    "            sagemaker_session=sagemaker_session\n",
    "        )\n",
    "        print(f\"Total rows: {total_rows}; Offset: {offset}\")\n",
    "        pickup_features = read_feature_data(limit, offset)\n",
    "        print(f'Ingesting data into feature group: {pickup_fg.name} ...')\n",
    "        pickup_fg.ingest(data_frame=pickup_features, max_processes=16, wait=True)\n",
    "        print(f'{len(pickup_features)} customer records ingested into feature group: {pickup_fg.name}')\n",
    "        offset += limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca35bbe2-a6d4-4559-ae3f-0b0b01576164",
   "metadata": {},
   "source": [
    "# Benchmark Reads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adae93c7-0a48-43a7-8cb2-bb5e3f5bb18e",
   "metadata": {},
   "source": [
    "## In-Memory Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c34e19-7c6b-4c8a-9a90-fdbc744add1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_store import FeatureStore\n",
    "\n",
    "feature_store = FeatureStore(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92a6d5-bb6b-4fbe-a802-e02665eace4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = 5\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_df, query = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training_{sf}'\n",
    ").to_dataframe()\n",
    "\n",
    "print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e72ca-4f7d-45ac-9a96-1ca3b664992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 10\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_df, query = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training_{sf}'\n",
    ").to_dataframe()\n",
    "\n",
    "print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68d156-296a-4dd0-bcc4-8a4130029015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 20\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_df, query = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training_{sf}'\n",
    ").to_dataframe()\n",
    "\n",
    "print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4e076-3d62-4ce0-83ee-59d05332ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 50\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_df, query = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training_{sf}'\n",
    ").to_dataframe()\n",
    "\n",
    "print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_df.count()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d2e911d-e653-4a3c-8192-4c37661dd6c1",
   "metadata": {},
   "source": [
    "## Training Dataset to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f21679-e844-433a-b996-e97e738f9d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = 5\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dataset_builder = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training/test_{sf}'\n",
    ")\n",
    "\n",
    "result_path, query = dataset_builder.include_duplicated_records().to_csv_file()\n",
    "\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c2c23-1d16-4812-809a-cbd9efb14755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = 10\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dataset_builder = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training/test_{sf}'\n",
    ")\n",
    "\n",
    "result_path, query = dataset_builder.include_duplicated_records().to_csv_file()\n",
    "\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50703b66-de28-4d3a-8237-495e724e3c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = 20\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dataset_builder = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training/test_{sf}'\n",
    ")\n",
    "\n",
    "result_path, query = dataset_builder.include_duplicated_records().to_csv_file()\n",
    "\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9620e53-2938-4373-b790-93c2ba76e9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = 50\n",
    "pickup_features_group_name = f'pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dataset_builder = feature_store.create_dataset(\n",
    "   base=pickup_fg,\n",
    "   output_path=f's3://{default_bucket}/{prefix}/training/test_{sf}'\n",
    ")\n",
    "\n",
    "result_path, query = dataset_builder.include_duplicated_records().to_csv_file()\n",
    "\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data:\\n {result_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
