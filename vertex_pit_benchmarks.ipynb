{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9ca04-fb7f-4856-aa8d-93bc6c39df1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install duckdb --pre && pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d65af7-09e3-4fac-aec1-51998e52d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import duckdb\n",
    "import boto3\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505de94-252d-4fe2-ab43-d6852dc204f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06060840-2d07-4077-84cf-35cbd177070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config get project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de651f-5176-4f27-8712-79cfb8e82d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"\n",
    "\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7274538-8607-4b68-93ca-91f34c5e5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west1\"  # @param {type:\"string\"}\n",
    "FEATURESTORE_ID = \"taxidata_fs\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f6c0b-5b1e-432e-8434-259738ee8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1ac7c-3b60-4fdf-bdeb-5e6f31f67022",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"35GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"pit-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n",
    "session = boto3.Session(aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096700c8-7dac-457e-a182-9989d5153599",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7d233-4219-4486-b2b8-1646e16aef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.close()\n",
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2f1db-39d2-4e3d-8650-8139963af49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tables as a dataframe\n",
    "con.execute(\"PRAGMA threads=16\")\n",
    "con.execute(\"SET preserve_insertion_order=false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9691dde-2bfb-42e5-aa9f-d29d71c36588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature_data(limit, offset):\n",
    "    lim = limit\n",
    "    off = offset\n",
    "    query = f'''\n",
    "        CREATE \n",
    "        OR REPLACE VIEW taxidata \n",
    "        AS\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            read_parquet([\n",
    "                's3://{BUCKET}/taxidata_cleaned/2011.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2012.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "            ])\n",
    "    '''\n",
    "    con.execute(query)\n",
    "    raw_data = con.execute(f\"SELECT * FROM taxidata LIMIT {lim} OFFSET {off}\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    raw_data = raw_data.astype({\"row_id\": \"string\"})\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f88a64-5b82-45f2-9bd0-dc85754bc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create featurestore\n",
    "try:\n",
    "    taxidata_feature_store = aiplatform.Featurestore(\n",
    "        featurestore_name='taxidata_fs'\n",
    "    )\n",
    "    print(\"Featurestore already exists...\")\n",
    "except:\n",
    "    print(\"Featurestore not found, creating it instead...\")\n",
    "    taxidata_feature_store = aiplatform.Featurestore.create(\n",
    "        featurestore_id=\"taxidata_fs\",\n",
    "        online_store_fixed_node_count=0\n",
    "    )\n",
    "    \n",
    "taxidata_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33483ec3-2b28-45e6-b814-afd7980354db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickup features entity type\n",
    "try:\n",
    "    print(\"pickup feature entity already exists...\")\n",
    "    pickup_fg_entity_type = taxidata_feature_store.get_entity_type(\n",
    "        entity_type_id=\"pickup_fg_entity_type\",\n",
    "    )\n",
    "except:\n",
    "    print(\"pickup feature entity type not found, creating it instead...\")\n",
    "    pickup_fg_entity_type = taxidata_feature_store.create_entity_type(\n",
    "        entity_type_id=\"pickup_fg_entity_type\",\n",
    "        description=\"Pickup features entity type\",\n",
    "    )\n",
    "\n",
    "        \n",
    "# Create dropoff features entity type\n",
    "try:\n",
    "    print(\"dropoff feature entity already exists...\")\n",
    "    dropoff_fg_entity_type = taxidata_feature_store.get_entity_type(\n",
    "        entity_type_id=\"dropoff_fg_entity_type\",\n",
    "    )\n",
    "except:\n",
    "    print(\"dropoff feature entity type not found, creating it instead...\")\n",
    "    dropoff_fg_entity_type = taxidata_feature_store.create_entity_type(\n",
    "        entity_type_id=\"dropoff_fg_entity_type\",\n",
    "        description=\"Dropoff features entity type\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815fc08-ac76-4048-81ae-9fff5f0cc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"pu_row_id\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"pu_row_id\",\n",
    "    },\n",
    "'''\n",
    "pickup_fg_config = {\n",
    "    \"pu_location_id\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"Pickup location ID\",\n",
    "    },\n",
    "    \"pu_borough\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"Pickup borough\",\n",
    "    },\n",
    "    \"mean_fare_window_1h_pickup_zip\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"Mean fare pickup window 1 hour\",\n",
    "    },\n",
    "    \"count_trips_window_1h_pickup_zip\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"Count trips pickup window 1 hour\",\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    pickup_fg_entity = pickup_fg_entity_type.batch_create_features(\n",
    "        feature_configs=pickup_fg_config,\n",
    "        sync = True\n",
    "    )\n",
    "    print(\"Entity feature group definition created\")\n",
    "except:\n",
    "    print(\"Entity feature group definition already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae75997-972e-4735-81a4-6ff31421db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"do_row_id\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"do_row_id\",\n",
    "    },\n",
    "'''\n",
    "dropoff_fg_config = {\n",
    "    \"do_location_id\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"Dropoff location ID\",\n",
    "    },\n",
    "    \"do_borough\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"Dropoff borough\",\n",
    "    },\n",
    "    \"dropoff_is_weekend\": {\n",
    "        \"value_type\": \"BOOL\",\n",
    "        \"description\": \"Dropoff is a weekend or not\",\n",
    "    },\n",
    "    \"count_trips_window_30m_dropoff_zip\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"Count trips dropoff window 30 min\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    dropoff_fg_entity = dropoff_fg_entity_type.batch_create_features(\n",
    "        feature_configs=dropoff_fg_config,\n",
    "        sync = True\n",
    "    )\n",
    "    print(\"Entity feature group definition created\")\n",
    "except:\n",
    "    print(\"Entity feature group definition already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69cb96-4fae-422b-912c-6bc505876280",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5000000\n",
    "offset = 0\n",
    "raw_data = read_feature_data(limit, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c92b34-19e9-4212-a082-0e7e7e92a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c39804-8b3f-4933-b888-87791a8ae519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_ts(df, ts_column, start_date, end_date):\n",
    "    if ts_column and start_date:\n",
    "        df = df[df[ts_column] >= start_date]\n",
    "    if ts_column and end_date:\n",
    "        df = df[df[ts_column] < end_date]\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_pickup_datetime']).dt.floor('15min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    pickup_features = (\n",
    "        df.groupby(['pu_location_id', 'pu_borough', 'window'])\n",
    "        .agg(\n",
    "            mean_fare_window_1h_pickup_zip=('fare_amount', 'mean'),\n",
    "            count_trips_window_1h_pickup_zip=('fare_amount', 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'window': 'ts'})\n",
    "    )\n",
    "    pickup_features['row_id'] = pickup_features.reset_index().index\n",
    "    row_id = pickup_features.pop('row_id')\n",
    "    pickup_features.insert(0, 'row_id', row_id)\n",
    "    pickup_features.rename(columns={'row_id':'pu_row_id'}, inplace = True)\n",
    "\n",
    "    return pickup_features\n",
    "\n",
    "def dropoff_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_dropoff_datetime']).dt.floor('30min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    dropoff_features = (\n",
    "        df.groupby(['do_location_id', 'do_borough', 'window'])\n",
    "        .agg(count_trips_window_30m_dropoff_zip=('do_borough', 'count'))\n",
    "        .reset_index()\n",
    "        .rename(columns={'window': 'ts'})\n",
    "    )\n",
    "    dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts'])\n",
    "    dropoff_features['dropoff_is_weekend'] = dropoff_features['ts'].dt.dayofweek.isin([5, 6])\n",
    "    dropoff_features['row_id'] = dropoff_features.reset_index().index\n",
    "    row_id = dropoff_features.pop('row_id')\n",
    "    dropoff_features.insert(0, 'row_id', row_id)\n",
    "    dropoff_features.rename(columns={'row_id':'do_row_id'}, inplace = True)\n",
    "\n",
    "    return dropoff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08578cc1-226d-4cbe-9bf4-ad7d0ed03c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "pickup_features = pickup_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_pickup_datetime\",\n",
    "    start_date=datetime(2011, 1, 1),\n",
    "    end_date=datetime(2023, 1, 31),\n",
    ")\n",
    "pickup_features['ts'] = pd.to_datetime(pickup_features['ts'], format='%Y-%m-%d %H:%M:%S').astype('datetime64[ns, UTC]')\n",
    "pickup_features = pickup_features.astype({\"pu_row_id\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201c50d-3ddc-425a-bbb7-7a66412c4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7aac2-92ad-4023-a83c-9ed10f21aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_features = dropoff_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_dropoff_datetime\",\n",
    "    start_date=datetime(2011, 1, 1),\n",
    "    end_date=datetime(2023, 1, 31),\n",
    ")\n",
    "\n",
    "dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts'], format='%Y-%m-%d %H:%M:%S').astype('datetime64[ns, UTC]')\n",
    "dropoff_features = dropoff_features.astype({\"do_row_id\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421876c-a53b-4769-9e41-e3e1c2a49bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d748fd2-2637-4a9b-8e49-5f85ad74fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKUP_FEAT_IDS = [feature.name for feature in pickup_fg_entity_type.list_features()]\n",
    "DROPOFF_FEAT_IDS = [feature.name for feature in dropoff_fg_entity_type.list_features()]\n",
    "PICKUP_FEAT_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac992f-be48-4078-b46a-2331f9369e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fg_entity_type.ingest_from_df(\n",
    "    feature_ids=PICKUP_FEAT_IDS,\n",
    "    feature_time=\"ts\",\n",
    "    entity_id_field=\"pu_row_id\",\n",
    "    df_source=pickup_features\n",
    ")\n",
    "\n",
    "dropoff_fg_entity_type.ingest_from_df(\n",
    "    feature_ids=DROPOFF_FEAT_IDS,\n",
    "    feature_time=\"ts\",\n",
    "    entity_id_field=\"do_row_id\",\n",
    "    df_source=dropoff_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6cce1-9fd5-45f4-b4d3-8015a949a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = raw_data.pop('row_id')\n",
    "raw_data.insert(0, 'pu_row_id', row_id)\n",
    "raw_data.insert(1, 'do_row_id', row_id)\n",
    "raw_data = raw_data.astype({\"pu_row_id\": \"string\"})\n",
    "raw_data = raw_data.astype({\"do_row_id\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4591e5f-5951-4532-a128-a04781e8cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.rename(columns = {'tpep_pickup_datetime':'timestamp'}, inplace = True)\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bf12b-96e0-4fe5-8ba8-036982b295ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FEATURE_IDS = {\n",
    "    \"pickup_fg_entity_type\": [\"pu_location_id\", \"pu_borough\", \"mean_fare_window_1h_pickup_zip\", \"count_trips_window_1h_pickup_zip\"],\n",
    "    \"dropoff_fg_entity_type\": [\"do_location_id\", \"do_borough\", \"count_trips_window_30m_dropoff_zip\", \"dropoff_is_weekend\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd76f9-3753-49d1-a39e-ccebf1b0e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_inst_df = raw_data.copy()\n",
    "read_inst_df.drop('tpep_dropoff_datetime', axis=1, inplace=True)\n",
    "read_inst_df.drop('pu_location_id', axis=1, inplace=True)\n",
    "read_inst_df.drop('do_location_id', axis=1, inplace=True)\n",
    "read_inst_df.drop('pu_borough', axis=1, inplace=True)\n",
    "read_inst_df.drop('do_borough', axis=1, inplace=True)\n",
    "read_inst_df.drop('pu_svc_zone', axis=1, inplace=True)\n",
    "read_inst_df.drop('do_svc_zone', axis=1, inplace=True)\n",
    "read_inst_df.drop('pu_zone', axis=1, inplace=True)\n",
    "read_inst_df.drop('do_zone', axis=1, inplace=True)\n",
    "read_inst_df.rename(columns={'pu_row_id':'pickup_fg_entity_type', 'do_row_id':'dropoff_fg_entity_type'}, inplace = True)\n",
    "ts = read_inst_df.pop('timestamp')\n",
    "read_inst_df.insert(len(read_inst_df.columns), 'timestamp', ts)\n",
    "print(\"before: \", read_inst_df['timestamp'].dtype)\n",
    "read_inst_df['timestamp'] = pd.to_datetime(read_inst_df['timestamp'], format='%Y-%m-%dT%H:%M:%SZ').astype('datetime64[ns, UTC]')\n",
    "print(\"after:  \", read_inst_df['timestamp'].dtype)\n",
    "read_inst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb2a33-6b5e-460e-b992-ffb77c647543",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "training_df = taxidata_feature_store.batch_serve_to_df(\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_df=read_inst_df,\n",
    "    pass_through_fields=[\"trip_distance\", \"fare_amount\", \"tip_amount\"],\n",
    ")\n",
    "print(f\"Time taken for in-memory PIT Join training data: {time.time() - start}\")\n",
    "\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cdb8c-28b8-4bcc-9b1d-08439dc7e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41f04c-5935-4450-ba11-8338f365117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create read-instances csv file from raw_data\n",
    "\n",
    "READ_INSTANCES_CSV = 'read-instances-5m.csv'\n",
    "\n",
    "# cols = ['pu_row_id','do_row_id','trip_distance','fare_amount','tip_amount','timestamp']\n",
    "read_inst_df.to_csv(READ_INSTANCES_CSV, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99638af3-3cb5-4abb-9bd3-cb109d610075",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp read-instances-5m.csv 'gs://ayush-bench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335969b-4433-4a45-9f2b-5b147b4d7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 5 lines of CSV\n",
    "import csv\n",
    "\n",
    "with open(READ_INSTANCES_CSV) as file:\n",
    "    count = 0\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if count < 5:\n",
    "            print(row)\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c39c9e-cc18-434d-bae9-90340d8606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "READ_INSTANCES_CSV_URI = f'gs://ayush-bench/{READ_INSTANCES_CSV}'\n",
    "\n",
    "# Output dataset\n",
    "DESTINATION_DATA_SET = \"taxidata\"  # @param {type:\"string\"}\n",
    "VERSION = \"v1\"\n",
    "DESTINATION_DATA_SET = \"{prefix}_{version}\".format(\n",
    "    prefix=DESTINATION_DATA_SET, version=VERSION\n",
    ")\n",
    "\n",
    "# Output table. Make sure that the table does NOT already exist; the BatchReadFeatureValues API cannot overwrite an existing table\n",
    "DESTINATION_TABLE_NAME = \"taxidata_pit_join_training_data\"  # @param {type:\"string\"}\n",
    "\n",
    "DESTINATION_PATTERN = \"bq://{project}.{dataset}.{table}\"\n",
    "DESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=DESTINATION_TABLE_NAME\n",
    ")\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset_id = \"{}.{}\".format(client.project, DESTINATION_DATA_SET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = REGION\n",
    "dataset = client.create_dataset(dataset)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "training_df = taxidata_feature_store.batch_serve_to_bq(\n",
    "    bq_destination_output_uri=DESTINATION_TABLE_URI,\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=READ_INSTANCES_CSV_URI,\n",
    ")\n",
    "print(f\"Time taken for BigQuery write PIT Join training data: {time.time() - start}\")\n",
    "\n",
    "training_df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python39",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
