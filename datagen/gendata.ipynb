{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3132b50",
   "metadata": {},
   "source": [
    "# Setup HIVE-style Partitioned Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import os\n",
    "import duckdb\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6974cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"25GB\" # increase to available python memory -25%\n",
    "DUCKDB_FILE = \"data/gendata.duckdb\"\n",
    "TMP_DIR = \"data/\"\n",
    "DATA_FOLDER = \"data/exp5\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "BUCKET = \"ayushman-hops\"\n",
    "\n",
    "# HDFS Uploads\n",
    "HOPS_HOST=''\n",
    "HOPS_API_KEY=''\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n",
    "# !ls -lR data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendata(ROWS, PARTITIONS):\n",
    "    with duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY,\n",
    "                                              'temp_directory': TMP_DIR}) as con:\n",
    "        con.execute(\"DROP TABLE IF EXISTS gendata\")\n",
    "\n",
    "        id_cols = ', '.join([f\"CAST(floor(random()*{card}) AS INT) as id{i}\" \n",
    "                             for (i, card) in enumerate(PARTITIONS+[10]*(round(COLS/4)-len(PARTITIONS)))])\n",
    "        float_cols = ', '.join([f\"random() as rand{i}\" for i in range(round(COLS/4))])\n",
    "        string_cols = ', '.join([f\"md5('{''.join(random.sample(string.ascii_letters,20))}') as str_hash{i}\" \n",
    "                                 for i in range(round(COLS/4))])\n",
    "        dt_cols = ', '.join([f\"to_timestamp({''.join(random.sample(string.digits,9))}) as dt{i}\" \n",
    "                             for i in range(round(COLS/4))])\n",
    "\n",
    "        con.execute(f\"\"\"CREATE TABLE gendata AS (SELECT {id_cols}, \n",
    "                                                        {float_cols},\n",
    "                                                        {string_cols},\n",
    "                                                        {dt_cols}\n",
    "                                                 FROM range({ROWS}) tbl(x));\"\"\")\n",
    "\n",
    "        df = con.execute(\"SELECT * FROM gendata LIMIT 10\").fetchdf()\n",
    "    #df\n",
    "\n",
    "#gendata(ROWS, PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS=20\n",
    "PARTITIONS=[2]\n",
    "id_cols = ', '.join([f\"CAST(floor(random()*{card}) AS INT) as id{i}\" \n",
    "                             for (i, card) in enumerate(PARTITIONS+[10]*(round(COLS/4)-len(PARTITIONS)))])\n",
    "print(id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(ROW_SHORT, export=False):\n",
    "    with duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY,\n",
    "                                             'temp_directory': TMP_DIR}) as con:\n",
    "        con.execute(f\"SET threads='{FILES_PER_PARTITION}';\")\n",
    "        con.execute(f\"SET preserve_insertion_order=false;\")\n",
    "        con.execute(f\"PRAGMA memory_limit='{MAX_MEMORY}';\")\n",
    "        #con.register_filesystem(hopsfs_fsspec)\n",
    "\n",
    "        WRITE_PATH = f\"{DATA_FOLDER}/{ROW_SHORT}/genpart{len(PARTITIONS)}/\"\n",
    "        print(WRITE_PATH)\n",
    "\n",
    "        part_cols = ', '.join([f\"id{i}\" for (i, card) in enumerate(PARTITIONS)])\n",
    "        if export:\n",
    "            con.execute(f\"\"\"EXPORT DATABASE '{WRITE_PATH}' (FORMAT PARQUET);\"\"\")\n",
    "        else:\n",
    "            if PARTITION:\n",
    "                con.execute(f\"\"\"COPY (SELECT * FROM gendata) TO '{WRITE_PATH}' \n",
    "                        (FORMAT PARQUET, \n",
    "                        PARTITION_BY ({part_cols}), \n",
    "                        ROW_GROUP_SIZE {ROW_GROUP_SIZE},\n",
    "                        ALLOW_OVERWRITE TRUE)\"\"\")\n",
    "            else:\n",
    "                for i in range(FILES_PER_PARTITION):\n",
    "                    WRITE_PATH = f\"{DATA_FOLDER}/{ROW_SHORT}/genpart{len(PARTITIONS)}/data_{i}.parquet\"\n",
    "                    con.execute(f\"\"\"COPY (SELECT * FROM gendata) TO '{WRITE_PATH}' \n",
    "                            (FORMAT PARQUET, \n",
    "                            ROW_GROUP_SIZE {ROW_GROUP_SIZE},\n",
    "                            ALLOW_OVERWRITE TRUE)\"\"\")\n",
    "#write_parquet(ROW_SHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(WRITE_PATH)\n",
    "# !ls -l {WRITE_PATH}\n",
    "# #!ls -la /tmp/gendata/rowsize20m/genpart3/id0=0/id1=0/id2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bea11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def upload_s3(path):\n",
    "    session = boto3.Session(\n",
    "       aws_access_key_id=AWS_ACCESS_KEY,\n",
    "       aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    s3 = boto3.client('s3')\n",
    "    print(f'Uploading {path} to S3')\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            s3.upload_file(os.path.join(root,file), BUCKET, os.path.join(root,file))\n",
    "    print(f'Finished uploading to S3...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import hopsworks\n",
    "from hops import hdfs\n",
    "\n",
    "def copy_to_hdfs(src_path):\n",
    "    project = hopsworks.login(host=HOPS_HOST,\n",
    "                                  port=443,\n",
    "                                  api_key_value=HOPS_API_KEY)\n",
    "    target_path = HDFS_PATH + src_path + '/../' #step back one directory so it doesn't create two same directories\n",
    "    if(not hdfs.exists(target_path)):\n",
    "        hdfs.mkdir(target_path)\n",
    "    print(f'Uploading {src_path} to HDFS...')\n",
    "    hdfs.copy_to_hdfs(src_path, target_path, overwrite=True)\n",
    "    print(f'Finished uploading to HDFS...')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2dd50a8",
   "metadata": {},
   "source": [
    "## Generate Many Partitions in Many Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27046cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''''''''''''''''''''''''\n",
    "'''''''''''''''''''''''''''\n",
    "''' Start of generator  '''\n",
    "'''''''''''''''''''''''''''\n",
    "'''''''''''''''''''''''''''\n",
    "\n",
    "ROWS = 42000000\n",
    "ROW_SHORT = \"16parts\"\n",
    "PARTITIONS = [16]\n",
    "PARTITION = True # Set to False to avoid hive-style partitioning\n",
    "ROW_GROUP_SIZE = 1000000\n",
    "S3 = True  # S3 Uploads\n",
    "HOPSFS = True  # HDFS Uploads\n",
    "COLS = 20 # number of columns, 50% integer, 50% float\n",
    "FILES_PER_PARTITION = 1 # files per partition/number of writer threads\n",
    "EXPORT = False # Export the whole DB as one parquet file\n",
    "\n",
    "if len(PARTITIONS) > round(COLS/2):\n",
    "    print(f\"Only half of the colums are reserved for id columns, please reduce the number of partition keys to <= {round(COLS/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ff7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n",
    "\n",
    "\n",
    "for i in range(len(PARTITIONS)):\n",
    "    if not os.path.exists(f\"{DATA_FOLDER}/{ROW_SHORT}/\"):\n",
    "        os.mkdir(f\"{DATA_FOLDER}/{ROW_SHORT}/\")\n",
    "    if not os.path.exists(f\"{DATA_FOLDER}/{ROW_SHORT}/genpart{i+1}/\"):\n",
    "        os.mkdir(f\"{DATA_FOLDER}/{ROW_SHORT}/genpart{i+1}/\")\n",
    "\n",
    "WRITE_PATH = f\"{DATA_FOLDER}/{ROW_SHORT}/genpart{len(PARTITIONS)}\"\n",
    "print(WRITE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f816523",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {DATA_FOLDER}/\n",
    "!ls -lh data/exp5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac397b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendata(ROWS, PARTITIONS)\n",
    "write_parquet(ROW_SHORT, EXPORT)\n",
    "\n",
    "if S3:\n",
    "    upload_s3(WRITE_PATH)\n",
    "if HOPSFS:\n",
    "    copy_to_hdfs(WRITE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WRITE_PATH)\n",
    "!ls -lh {WRITE_PATH}/id0=0/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a62b1e55",
   "metadata": {},
   "source": [
    "# Generate Many Partitions in Single Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from hops import hdfs\n",
    "#from pydoop import hdfs\n",
    "#from fsspec.implementations.arrow import HadoopFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_FILES = 100000\n",
    "GEN_PATH = f'data/gen100k'\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d65798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {GEN_PATH}\n",
    "\n",
    "for i in range(NUMBER_OF_FILES):\n",
    "    if (not os.path.exists(f\"{GEN_PATH}/part_{str(i)}.test\")):\n",
    "        with open(f\"{GEN_PATH}/part_{str(i)}.test\", mode=\"w\") as f:\n",
    "            f.write(\"\")\n",
    "            f.close()\n",
    "\n",
    "#upload_s3(GEN_PATH)\n",
    "copy_to_hdfs(GEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530711d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = GEN_PATH\n",
    "project = hopsworks.login(host=HOPS_HOST,\n",
    "                              port=443,\n",
    "                              api_key_value=HOPS_API_KEY)\n",
    "target_path = HDFS_PATH + src_path + '/../' #step back one directory so it doesn't create two same directories\n",
    "if(not hdfs.exists(target_path)):\n",
    "    hdfs.mkdir(target_path)\n",
    "print(f'Uploading {src_path} to HDFS...')\n",
    "hdfs.copy_to_hdfs(src_path, target_path, overwrite=True)\n",
    "print(f'Finished uploading to HDFS...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
