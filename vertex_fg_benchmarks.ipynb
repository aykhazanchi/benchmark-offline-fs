{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9ca04-fb7f-4856-aa8d-93bc6c39df1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install duckdb --pre --upgrade && pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d65af7-09e3-4fac-aec1-51998e52d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import duckdb\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505de94-252d-4fe2-ab43-d6852dc204f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06060840-2d07-4077-84cf-35cbd177070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config get project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de651f-5176-4f27-8712-79cfb8e82d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\n",
    "\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7274538-8607-4b68-93ca-91f34c5e5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west1\"  # @param {type:\"string\"}\n",
    "FEATURESTORE_ID = \"taxidata_fs\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f6c0b-5b1e-432e-8434-259738ee8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1ac7c-3b60-4fdf-bdeb-5e6f31f67022",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"35GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"fg-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n",
    "session = boto3.Session(aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096700c8-7dac-457e-a182-9989d5153599",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7d233-4219-4486-b2b8-1646e16aef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.close()\n",
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2f1db-39d2-4e3d-8650-8139963af49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tables as a dataframe\n",
    "con.execute(\"PRAGMA threads=16\")\n",
    "con.execute(\"SET preserve_insertion_order=false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9691dde-2bfb-42e5-aa9f-d29d71c36588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature_data(limit, offset):\n",
    "    lim = limit\n",
    "    off = offset\n",
    "    query = f'''\n",
    "        CREATE \n",
    "        OR REPLACE VIEW taxidata \n",
    "        AS\n",
    "        SELECT \n",
    "            tpep_pickup_datetime, \n",
    "            pu_location_id, \n",
    "            pu_borough,\n",
    "            pu_svc_zone,\n",
    "            pu_zone \n",
    "        FROM \n",
    "            read_parquet([\n",
    "                's3://{BUCKET}/taxidata_cleaned/2011.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2012.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "            ])\n",
    "    '''\n",
    "    con.execute(query)\n",
    "    raw_data = con.execute(f\"SELECT * FROM taxidata LIMIT {lim} OFFSET {off}\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f88a64-5b82-45f2-9bd0-dc85754bc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create featurestore\n",
    "try:\n",
    "    taxidata_feature_store = aiplatform.Featurestore(\n",
    "        featurestore_name='taxidata_fs'\n",
    "    )\n",
    "    print(\"Featurestore already exists...\")\n",
    "except:\n",
    "    print(\"Featurestore not found, creating it instead...\")\n",
    "    taxidata_feature_store = aiplatform.Featurestore.create(\n",
    "        featurestore_id=\"taxidata_fs\",\n",
    "        online_store_fixed_node_count=0\n",
    "    )\n",
    "    \n",
    "taxidata_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33483ec3-2b28-45e6-b814-afd7980354db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_fg_entity_type(sf):\n",
    "    entity_type_name = f\"pickup_read_entity_type_{sf}\"\n",
    "    try:\n",
    "        pickup_fg_entity_type = taxidata_feature_store.get_entity_type(\n",
    "            entity_type_id=entity_type_name,\n",
    "        )\n",
    "        print(\"pickup feature entity already exists...\")\n",
    "    except:\n",
    "        print(\"pickup feature entity type not found, creating it instead...\")\n",
    "        pickup_fg_entity_type = taxidata_feature_store.create_entity_type(\n",
    "            entity_type_id=entity_type_name,\n",
    "            description=\"Pickup features entity type\",\n",
    "        )\n",
    "        print(\"Found FG entity type\", entity_type_name)\n",
    "        # Create feature group config for newly created FG entity type\n",
    "        create_fg_config(pickup_fg_entity_type)\n",
    "    \n",
    "    return pickup_fg_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815fc08-ac76-4048-81ae-9fff5f0cc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fg_config = {\n",
    "    \"pu_location_id\": {\n",
    "        \"value_type\": \"INT64\",\n",
    "        \"description\": \"Pickup location ID\",\n",
    "    },\n",
    "    \"pu_borough\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"Pickup borough\",\n",
    "    },\n",
    "    \"pu_svc_zone\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"Pickup service zone\",\n",
    "    },\n",
    "    \"pu_zone\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"Pickup zone\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def create_fg_config(pickup_fg_entity_type):\n",
    "    try:\n",
    "        pickup_fg_entity = pickup_fg_entity_type.batch_create_features(\n",
    "            feature_configs=pickup_fg_config,\n",
    "            sync = True\n",
    "        )\n",
    "        print(\"Entity feature group definition created\")\n",
    "    except:\n",
    "        print(\"Entity feature group definition already exists\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61ce1fd0-712a-4bd6-9947-034083793197",
   "metadata": {},
   "source": [
    "## Ingest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f377314-6c2e-4133-8de6-36f6efe85f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = [50] # Number of millions of rows to scale \n",
    "limit = 5000000 # Get 5M at a time so it's faster\n",
    "\n",
    "for sf in scale_factor:\n",
    "    offset=0\n",
    "    total_rows = sf * 1000000  # Millions\n",
    "    while offset < total_rows:\n",
    "        print(f\"Total rows: {total_rows}; Offset: {offset}\")\n",
    "        # Compute the pickup features from raw data\n",
    "        pickup_features = read_feature_data(limit, offset)\n",
    "        pickup_features = pickup_features.astype({\"row_id\": \"string\"})\n",
    "        pickup_features['tpep_pickup_datetime'] = pd.to_datetime(pickup_features['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M:%S').astype('datetime64[ns, UTC]')\n",
    "        # Get the FG entity type\n",
    "        pickup_fg_entity_type = get_or_create_fg_entity_type(sf)\n",
    "        PICKUP_FEAT_IDS = [feature.name for feature in pickup_fg_entity_type.list_features()]\n",
    "        print(f\"Inserting into FG entity type: {pickup_fg_entity_type} - {sf}\")\n",
    "        pickup_fg_entity_type.ingest_from_df(\n",
    "            feature_ids=PICKUP_FEAT_IDS,\n",
    "            feature_time=\"tpep_pickup_datetime\",\n",
    "            entity_id_field=\"row_id\",\n",
    "            df_source=pickup_features\n",
    "        )\n",
    "        offset += limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69cb96-4fae-422b-912c-6bc505876280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37e1ae21-9be5-4907-b7ab-03e00766d6eb",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5175ce-bda2-4f72-a71d-da0b8133e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(sf):\n",
    "    query = f'''\n",
    "      SELECT \n",
    "          tpep_pickup_datetime,\n",
    "          trip_distance,\n",
    "          tip_amount,\n",
    "          fare_amount\n",
    "      FROM \n",
    "          read_parquet([\n",
    "            's3://{BUCKET}/taxidata_cleaned/2011.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2012.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "          ])\n",
    "      LIMIT {sf*1000000}\n",
    "    '''\n",
    "    raw_data = con.execute(query).df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data\n",
    "\n",
    "def transform_raw_data(sf, fg):\n",
    "    pickup_fg_entity_type = fg\n",
    "    raw_data = get_raw_data(sf)\n",
    "    raw_data = raw_data.astype({\"row_id\": \"string\"})\n",
    "    raw_data.rename(columns={'row_id':f'{pickup_fg_entity_type.name}'}, inplace = True)\n",
    "    ts = raw_data.pop('tpep_pickup_datetime')\n",
    "    raw_data.insert(len(raw_data.columns), 'timestamp', ts)\n",
    "    print(\"before: \", raw_data['timestamp'].dtype)\n",
    "    raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'], format='%Y-%m-%dT%H:%M:%SZ').astype('datetime64[ns, UTC]')\n",
    "    print(\"after:  \", raw_data['timestamp'].dtype)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6cce1-9fd5-45f4-b4d3-8015a949a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_with_data_in_memory(sf):\n",
    "    pickup_fg_entity_type = get_or_create_fg_entity_type(sf)\n",
    "    SERVING_FEATURE_IDS = {\n",
    "        f\"{pickup_fg_entity_type.name}\": [\"pu_location_id\", \"pu_borough\", \"pu_svc_zone\", \"pu_zone\"]\n",
    "    }\n",
    "    raw_data = transform_raw_data(sf, pickup_fg_entity_type)\n",
    "    start = time.time()\n",
    "    training_df = taxidata_feature_store.batch_serve_to_df(\n",
    "        serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "        read_instances_df=raw_data,\n",
    "        pass_through_fields=[\"trip_distance\", \"fare_amount\", \"tip_amount\"],\n",
    "    )\n",
    "    print(f\"Time taken for in-memory create of FG training data: {time.time() - start}\")\n",
    "\n",
    "    training_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df875095-8932-4fc5-887d-98b1256f5dde",
   "metadata": {},
   "source": [
    "# Create training data in memory\n",
    "### Note: In Vertex FS, even with in-memory, the data is first written to a BQ table and then served from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e39b3b6b-f531-4efb-9080-0e09fef06b36",
   "metadata": {},
   "source": [
    "## SF=5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4591e5f-5951-4532-a128-a04781e8cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=5\n",
    "run_benchmark_with_data_in_memory(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a7e0a44-d4ee-4a50-bd27-7f2bac2235b8",
   "metadata": {},
   "source": [
    "## SF=10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0be25-7c85-4031-99a0-30aeee857936",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=10\n",
    "run_benchmark_with_data_in_memory(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "232b4ebd-1d81-4c0d-b881-8c94ab2d4e9c",
   "metadata": {},
   "source": [
    "## SF=20M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bf12b-96e0-4fe5-8ba8-036982b295ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=20\n",
    "run_benchmark_with_data_in_memory(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c52d4e5-1c73-4e89-864b-403ddebb6c48",
   "metadata": {},
   "source": [
    "## SF=50M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb2a33-6b5e-460e-b992-ffb77c647543",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=50\n",
    "run_benchmark_with_data_in_memory(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9b206a-bdb5-4211-bddc-34dfc9e5c77c",
   "metadata": {},
   "source": [
    "# Create training data in BigQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1568db90-100a-4799-8197-8cce5b4f10cd",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41f04c-5935-4450-ba11-8338f365117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_read_instances_csv(sf):\n",
    "    # Create read-instances csv file from raw_data\n",
    "    READ_INSTANCES_CSV = f'ri-{sf}m.csv'\n",
    "\n",
    "    # cols = ['pu_row_id','do_row_id','trip_distance','fare_amount','tip_amount','timestamp']\n",
    "    raw_data.to_csv(READ_INSTANCES_CSV, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44900651-a13e-4a22-87d5-2a3b27878f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = [5, 10, 20, 50]\n",
    "for sf in scale_factor:\n",
    "    pickup_fg_entity_type = get_or_create_fg_entity_type(sf)\n",
    "    raw_data = transform_raw_data(sf, pickup_fg_entity_type)\n",
    "    create_read_instances_csv(sf)\n",
    "    raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99638af3-3cb5-4abb-9bd3-cb109d610075",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ri-*m.csv 'gs://ayush-bench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335969b-4433-4a45-9f2b-5b147b4d7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm if CSV uploaded correctly\n",
    "import csv\n",
    "\n",
    "sf=5\n",
    "READ_INSTANCES_CSV = f'ri-{sf}m.csv'\n",
    "\n",
    "with open(READ_INSTANCES_CSV) as file:\n",
    "    count = 0\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if count < 5:\n",
    "            print(row)\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c39c9e-cc18-434d-bae9-90340d8606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def run_bq_table_benchmark(sf):\n",
    "    READ_INSTANCES_CSV = f'ri-{sf}m.csv'\n",
    "    READ_INSTANCES_CSV_URI = f'gs://ayush-bench/{READ_INSTANCES_CSV}'\n",
    "\n",
    "    SERVING_FEATURE_IDS = {\n",
    "        f\"pickup_read_entity_type_{sf}\": [\"pu_location_id\", \"pu_borough\", \"pu_svc_zone\", \"pu_zone\"]\n",
    "    }\n",
    "\n",
    "    # Output dataset\n",
    "    DESTINATION_DATA_SET = \"taxidata\"  # @param {type:\"string\"}\n",
    "    VERSION = \"v1\"\n",
    "    DESTINATION_DATA_SET = \"{prefix}_{version}\".format(\n",
    "        prefix=DESTINATION_DATA_SET, version=VERSION\n",
    "    )\n",
    "\n",
    "    # Output table. Make sure that the table does NOT already exist; the BatchReadFeatureValues API cannot overwrite an existing table\n",
    "    DESTINATION_TABLE_NAME = f\"taxidata_{pickup_fg_entity_type.name}\"  # @param {type:\"string\"}\n",
    "\n",
    "    DESTINATION_PATTERN = \"bq://{project}.{dataset}.{table}\"\n",
    "    DESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "        project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=DESTINATION_TABLE_NAME\n",
    "    )\n",
    "\n",
    "\n",
    "    # Delete existing BigQuery dataset first as BatchReadFeatureValues API cannot overwrite an existing table\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    client.delete_dataset(\n",
    "        DESTINATION_DATA_SET, delete_contents=True, not_found_ok=True\n",
    "    )\n",
    "    print(\"Deleted dataset '{}'.\".format(DESTINATION_DATA_SET))\n",
    "\n",
    "\n",
    "    # Create dataset\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    dataset_id = \"{}.{}\".format(client.project, DESTINATION_DATA_SET)\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = REGION\n",
    "    try:\n",
    "        dataset = client.create_dataset(dataset)\n",
    "        print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "    except:    \n",
    "        print(\"Dataset {}.{} already exists. Delete did not work.\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "    start = time.time()\n",
    "    training_df = taxidata_feature_store.batch_serve_to_bq(\n",
    "        bq_destination_output_uri=DESTINATION_TABLE_URI,\n",
    "        serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "        read_instances_uri=READ_INSTANCES_CSV_URI,\n",
    "        pass_through_fields=[\"trip_distance\", \"fare_amount\", \"tip_amount\"],\n",
    "    )\n",
    "    print(f\"Time taken for BigQuery write of SF {sf}M FG training data: {time.time() - start}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf066499-03a7-480d-95b2-43b6c205cb51",
   "metadata": {},
   "source": [
    "## SF=5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66538f-c0b9-460f-bf37-5055e4999b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 5\n",
    "run_bq_table_benchmark(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7796cec5-0878-4d29-8174-4b71f532d111",
   "metadata": {},
   "source": [
    "## SF=10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88225f1-bd96-4a3b-85d2-65fd5265e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 10\n",
    "run_bq_table_benchmark(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f130ab5-80c5-49e5-96ff-9d90f99a02f8",
   "metadata": {},
   "source": [
    "## SF=20M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6641ab-2b09-4dae-b491-7e8b95c56bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 20\n",
    "run_bq_table_benchmark(sf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de5b77f0-85f4-4a78-90d6-33e10af6679f",
   "metadata": {},
   "source": [
    "## SF=50M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed96c3-fa40-4683-8be5-a1b4470a2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 50\n",
    "run_bq_table_benchmark(sf)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python39",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
