{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa41fb0-2cc3-45a3-bfd2-cba4de8ca94c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install duckdb --pre --upgrade && pip install --pre pandas==2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2572496-ac31-4941-a93f-11e91108de43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.inputs import TableFormatEnum\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import importlib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c7cd1-1af7-477b-b51c-e4a4e63c22e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_version = sagemaker.__version__\n",
    "major, minor, patch = sm_version.split('.')\n",
    "if int(major) < 2 or int(minor) < 125:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker==2.125.0'])\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed3ddd-ada1-4372-bf37-42100eac67aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5368b12-61f9-4a6e-91c6-c5395cd4fb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "logger.info(f'Default S3 bucket = {default_bucket}')\n",
    "prefix = 'sagemaker-feature-store'\n",
    "region = sagemaker_session.boto_region_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84825f50-e41f-4f77-acde-6827d0649adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"45GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='eu-west-1'\n",
    "BUCKET = \"ayushman-hops\"\n",
    "session = boto3.Session( aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# HDFS Uploads\n",
    "HOPS_HOST=''\n",
    "HOPS_API_KEY=''\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee18b0-7646-48d6-82f0-0be1e6114872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d9b0a-e765-4e1e-a21c-7eb7366956ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bae3c-90e9-46df-ad23-79369f49a138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        logger.info(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(10)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    logger.info(f'FeatureGroup {feature_group.name} was successfully created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78be0f-326a-47b8-9761-d249d5086084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "From Documentation:\n",
    "Amazon SageMaker Feature Store supports the AWS Glue and Apache Iceberg table formats for the offline store. \n",
    "You can choose the table format when youâ€™re creating a new feature group.\n",
    "\n",
    "Using Apache Iceberg for storing features accelerates model development by enabling faster query performance when extracting ML training datasets,\n",
    "taking advantage of Iceberg table compaction. Depending on the design of your feature groups and their scale, you can experience training query \n",
    "performance improvements of 10x to 100x by using this new capability.\n",
    "'''\n",
    "\n",
    "table_format_param = 'ICEBERG' # or 'GLUE'\n",
    "if table_format_param == 'ICEBERG':\n",
    "    table_format = TableFormatEnum.ICEBERG\n",
    "else:\n",
    "    table_format = TableFormatEnum.GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fc67e-c388-4350-b19f-ccd147b374b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_data(sf):\n",
    "    file_path=f's3://{BUCKET}/taxidata_cleaned/*.parquet'\n",
    "    limit = sf * 1000000\n",
    "    raw_data = con.execute(f\"SELECT * FROM read_parquet('{file_path}') LIMIT {limit};\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf494252-172d-4253-bd7b-08d044ab225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1\n",
    "\n",
    "raw_data = get_raw_data(sf)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0195a4-6536-45ff-a5dd-557b076e0601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_df_by_ts(df, ts_column, start_date, end_date):\n",
    "    if ts_column and start_date:\n",
    "        df = df[df[ts_column] >= start_date]\n",
    "    if ts_column and end_date:\n",
    "        df = df[df[ts_column] < end_date]\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_pickup_datetime']).dt.floor('15min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    pickup_features = (\n",
    "        df.groupby(['pu_location_id', 'pu_borough', 'window'])\n",
    "        .agg(\n",
    "            mean_fare_window_1h_pickup_zip=('fare_amount', 'mean'),\n",
    "            count_trips_window_1h_pickup_zip=('fare_amount', 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={'pu_location_id': 'location_id', 'pu_borough': 'borough', 'window': 'ts'})\n",
    "    )\n",
    "    pickup_features['row_id'] = pickup_features.reset_index().index\n",
    "    row_id = pickup_features.pop('row_id')\n",
    "    pickup_features.insert(0, 'row_id', row_id)\n",
    "    pickup_features.rename(columns={'row_id':'pu_row_id'}, inplace = True)\n",
    "    pickup_features.drop('borough', axis=1, inplace=True)\n",
    "\n",
    "    return pickup_features\n",
    "\n",
    "def dropoff_features_fn(df, ts_column, start_date, end_date):\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['window'] = pd.to_datetime(df['tpep_dropoff_datetime']).dt.floor('30min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    dropoff_features = (\n",
    "        df.groupby(['do_location_id', 'do_borough', 'window'])\n",
    "        .agg(count_trips_window_30m_dropoff_zip=('do_borough', 'count'))\n",
    "        .reset_index()\n",
    "        .rename(columns={'do_location_id': 'location_id', 'do_borough': 'borough', 'window': 'ts'})\n",
    "    )\n",
    "    dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts'])\n",
    "    dropoff_features['dropoff_is_weekend'] = dropoff_features['ts'].dt.dayofweek.isin([5, 6])\n",
    "    dropoff_features['row_id'] = dropoff_features.reset_index().index\n",
    "    row_id = dropoff_features.pop('row_id')\n",
    "    dropoff_features.insert(0, 'row_id', row_id)\n",
    "    dropoff_features.rename(columns={'row_id':'do_row_id'}, inplace = True)\n",
    "    dropoff_features.drop('borough', axis=1, inplace=True)\n",
    "\n",
    "    return dropoff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a4b32-43fd-4e91-b1c1-d3a089353e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickup_features = pickup_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_pickup_datetime\",\n",
    "    start_date=datetime(2011, 1, 1),\n",
    "    end_date=datetime(2023, 1, 31),\n",
    ")\n",
    "\n",
    "pickup_features['ts'] = pd.to_datetime(pickup_features['ts']).dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb445315-1dd7-4e57-ac42-af28dd96feb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickup_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc5bc3-219c-4631-8b6e-885de62770c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropoff_features = dropoff_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_dropoff_datetime\",\n",
    "    start_date=datetime(2011, 1, 1),\n",
    "    end_date=datetime(2023, 1, 31),\n",
    ")\n",
    "\n",
    "dropoff_features['ts'] = pd.to_datetime(dropoff_features['ts']).dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "dropoff_features[\"dropoff_is_weekend\"] = dropoff_features[\"dropoff_is_weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662c73b-8635-4b40-a32a-24289bc0043b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropoff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8c3f0-adea-442b-b738-8c08f07a3456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Pickup PIT FG schema definition\n",
    "\n",
    "pu_features_schema_df = pickup_features.head(5)\n",
    "pu_features_schema_df = pu_features_schema_df.astype({'ts': 'string'})\n",
    "pu_features_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431093b-06d8-4ff6-a75e-87577649c12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dropoff PIT FG schema definition\n",
    "\n",
    "do_features_schema_df = dropoff_features.head(5)\n",
    "do_features_schema_df = do_features_schema_df.astype({'ts': 'string'})\n",
    "do_features_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f49634-9bed-487e-b994-236f8df34d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pickup_features_group_name = f'pit_pickup_features_{sf}'\n",
    "pickup_features_group = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "pickup_features_group.load_feature_definitions(data_frame=pu_features_schema_df)\n",
    "try:\n",
    "    pickup_features_group.create(\n",
    "        s3_uri=f's3://{default_bucket}/{prefix}/pit',\n",
    "        record_identifier_name='pu_row_id',\n",
    "        event_time_feature_name='ts',\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "        table_format=table_format\n",
    "    )\n",
    "    wait_for_feature_group_creation_complete(pickup_features_group)\n",
    "except:\n",
    "    print(f\"Feature group {pickup_features_group_name} already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f872d-1507-4b2e-814d-139e73785bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dropoff_features_group_name = f'pit_dropoff_features_{sf}'\n",
    "dropoff_features_group = FeatureGroup(\n",
    "    name=dropoff_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "dropoff_features_group.load_feature_definitions(data_frame=do_features_schema_df)\n",
    "try:\n",
    "    dropoff_features_group.create(\n",
    "        s3_uri=f's3://{default_bucket}/{prefix}/pit',\n",
    "        record_identifier_name='do_row_id',\n",
    "        event_time_feature_name='ts',\n",
    "        role_arn=role,\n",
    "        enable_online_store=False,\n",
    "        table_format=table_format\n",
    "    )\n",
    "    wait_for_feature_group_creation_complete(dropoff_features_group)\n",
    "except:\n",
    "    print(f\"Feature group {dropoff_features_group_name} already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4220932-f7c1-4b2a-b299-e65ae738cd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickup_features_group_name = f'pit_pickup_features_{sf}'\n",
    "pickup_fg = FeatureGroup(\n",
    "    name=pickup_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f'Ingesting data into feature group: {pickup_fg.name} ...')\n",
    "pickup_fg.ingest(data_frame=pickup_features, max_processes=16, wait=True)\n",
    "print(f'{len(pickup_features)} customer records ingested into feature group: {pickup_fg.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d2b71-81b4-4c56-9465-f557aeb1be09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dropoff_features_group_name = f'pit_dropoff_features_{sf}'\n",
    "dropoff_fg = FeatureGroup(\n",
    "    name=dropoff_features_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f'Ingesting data into feature group: {dropoff_fg.name} ...')\n",
    "dropoff_fg.ingest(data_frame=dropoff_features, max_processes=16, wait=True)\n",
    "print(f'{len(dropoff_features)} customer records ingested into feature group: {dropoff_fg.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4599ab-4835-4569-aed7-87a7e83fe2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Helper function to create Spine DF\n",
    "\n",
    "def transform_spine_df(read_inst_df):\n",
    "    row_id = read_inst_df.pop('row_id')\n",
    "    read_inst_df.insert(0, 'row_id', row_id)\n",
    "    read_inst_df.insert(1, 'pu_row_id', row_id)\n",
    "    read_inst_df.insert(2, 'do_row_id', row_id)\n",
    "    read_inst_df.drop('tpep_dropoff_datetime', axis=1, inplace=True)\n",
    "    # read_inst_df.drop('pu_location_id', axis=1, inplace=True)\n",
    "    read_inst_df.rename(columns={'pu_location_id': 'location_id'}, inplace=True)\n",
    "    read_inst_df['location_id'] = read_inst_df['location_id'].astype('int64')\n",
    "    read_inst_df.drop('do_location_id', axis=1, inplace=True)\n",
    "    read_inst_df.drop('pu_borough', axis=1, inplace=True)\n",
    "    read_inst_df.drop('do_borough', axis=1, inplace=True)\n",
    "    read_inst_df.drop('pu_svc_zone', axis=1, inplace=True)\n",
    "    read_inst_df.drop('do_svc_zone', axis=1, inplace=True)\n",
    "    read_inst_df.drop('pu_zone', axis=1, inplace=True)\n",
    "    read_inst_df.drop('do_zone', axis=1, inplace=True)\n",
    "    ts = read_inst_df.pop('tpep_pickup_datetime')\n",
    "    read_inst_df.insert(len(read_inst_df.columns), 'ts', ts)\n",
    "    print(\"before: \", read_inst_df['ts'].dtype)\n",
    "    read_inst_df['ts'] = pd.to_datetime(read_inst_df['ts']).dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    #read_inst_df['ts'] = read_inst_df['ts'].astype('string')\n",
    "    print(\"after:  \", read_inst_df['ts'].dtype)\n",
    "    return read_inst_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca35bbe2-a6d4-4559-ae3f-0b0b01576164",
   "metadata": {},
   "source": [
    "# Benchmark PIT Correct JOIN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adae93c7-0a48-43a7-8cb2-bb5e3f5bb18e",
   "metadata": {},
   "source": [
    "## In-Memory Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c34e19-7c6b-4c8a-9a90-fdbc744add1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_store import FeatureStore\n",
    "\n",
    "feature_store = FeatureStore(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92a6d5-bb6b-4fbe-a802-e02665eace4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scale_factor = [1,2,5,10]\n",
    "scale_factor = [1]\n",
    "\n",
    "for sf in scale_factor:\n",
    "    # Create raw data frame for join\n",
    "    raw_data = get_raw_data(sf)\n",
    "    read_inst_df = transform_spine_df(raw_data)\n",
    "\n",
    "    pickup_features_group_name = f'pit_pickup_features_{sf}'\n",
    "    pickup_fg = FeatureGroup(\n",
    "        name=pickup_features_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    dropoff_features_group_name = f'pit_dropoff_features_{sf}'\n",
    "    dropoff_fg = FeatureGroup(\n",
    "        name=dropoff_features_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    dataset_builder = feature_store.create_dataset(\n",
    "        base=read_inst_df, \n",
    "        event_time_identifier_feature_name='ts', \n",
    "        record_identifier_feature_name='row_id',\n",
    "        output_path=f's3://{default_bucket}/{prefix}/pit/training_{sf}'\n",
    "        )\\\n",
    "        .with_feature_group(\n",
    "            feature_group=pickup_fg,\n",
    "            target_feature_name_in_base='location_id',\n",
    "            included_feature_names=['mean_fare_window_1h_pickup_zip','count_trips_window_1h_pickup_zip']\n",
    "        )\\\n",
    "        .with_feature_group(\n",
    "            feature_group=dropoff_fg,\n",
    "            target_feature_name_in_base='location_id',\n",
    "            included_feature_names=['count_trips_window_30m_dropoff_zip', 'dropoff_is_weekend']\n",
    "        )\\\n",
    "        .include_duplicated_records()\\\n",
    "        .point_in_time_accurate_join()\\\n",
    "        .with_number_of_records_from_query_results(number_of_records=limit)\\\n",
    "        .with_number_of_recent_records_by_record_identifier(number_of_recent_records=limit)\n",
    "\n",
    "    result_df, query = dataset_builder.to_dataframe()\n",
    "\n",
    "    print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "    print(f\"Num of rows of training data:\\n {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e72ca-4f7d-45ac-9a96-1ca3b664992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d2e911d-e653-4a3c-8192-4c37661dd6c1",
   "metadata": {},
   "source": [
    "## Training Dataset to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f21679-e844-433a-b996-e97e738f9d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_factor = [1,2,5,10]\n",
    "\n",
    "for sf in scale_factor:\n",
    "    # Create raw data frame for join\n",
    "    raw_data = get_raw_data(sf)\n",
    "    read_inst_df = transform_spine_df(raw_data)\n",
    "\n",
    "    pickup_features_group_name = f'pit_pickup_features_{sf}'\n",
    "    pickup_fg = FeatureGroup(\n",
    "        name=pickup_features_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    dropoff_features_group_name = f'pit_dropoff_features_{sf}'\n",
    "    dropoff_fg = FeatureGroup(\n",
    "        name=dropoff_features_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    dataset_builder = feature_store.create_dataset(\n",
    "        base=read_inst_df, \n",
    "        event_time_identifier_feature_name='ts', \n",
    "        record_identifier_feature_name='row_id',\n",
    "        output_path=f's3://{default_bucket}/{prefix}/pit/training_{sf}'\n",
    "        )\\\n",
    "        .with_feature_group(\n",
    "            feature_group=pickup_fg,\n",
    "            target_feature_name_in_base='location_id',\n",
    "            included_feature_names=['mean_fare_window_1h_pickup_zip','count_trips_window_1h_pickup_zip']\n",
    "        )\\\n",
    "        .with_feature_group(\n",
    "            feature_group=dropoff_fg,\n",
    "            target_feature_name_in_base='location_id',\n",
    "            included_feature_names=['count_trips_window_30m_dropoff_zip', 'dropoff_is_weekend']\n",
    "        )\\\n",
    "        .include_duplicated_records()\\\n",
    "        .point_in_time_accurate_join()\\\n",
    "        .with_number_of_records_from_query_results(number_of_records=limit)\\\n",
    "        .with_number_of_recent_records_by_record_identifier(number_of_recent_records=limit)\n",
    "\n",
    "    result_df, query = dataset_builder.to_csv_file()\n",
    "\n",
    "    print(f\"time for SF {sf}: {time.time() - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
